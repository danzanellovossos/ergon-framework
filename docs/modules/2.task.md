## Task Module

### Overview

The Task Module defines the computational core of the Ergon Framework. A **Task** is a pure orchestration unit responsible for business logic. It receives transactions, applies rules, interacts with services for data enrichment or transformation, and produces output transactions.

**Crucially, a Task is "thin":**

- It contains NO I/O logic.
- It contains NO protocol details.
- It contains NO retry or connection management logic.

All interaction with the external world is delegated to **Connectors** (for transaction flow) and **Services** (for all other interactions). This ensures that tasks are testable, deterministic, and purely focused on domain intent.

### Transactions and atomicity

A task is fundamentally a deterministic unit of computation that processes transactions. The transaction is the central abstraction of the entire framework. A transaction is a generic, transport-agnostic, schema-flexible representation of a single atomic unit of work. Atomicity in this context does not refer to “smallness” but to “indivisibility”: a transaction is the smallest unit of work that a task should ever treat as conceptually whole and inseparable. It can correspond to a single message from a queue, a single record from a file, or a single API payload, but it may equally represent an entire batch of records if the batch is logically indivisible at the business level.

### Core abstractions

The Tasks Module provides a set of core abstractions to support this model.

- **Task Metaclass/Decorator**: a shared mechanism that injects common lifecycle and initialization behavior into both synchronous and asynchronous base task classes.
- **BaseTask**: contract for synchronous tasks executed under multi-process and multi-thread strategies.
- **BaseAsyncTask**: equivalent contract for event-loop based asynchronous tasks.
- **Mixins**: provide composable behavioral roles: consumer, producer, and hybrid semantics.
- **Runners**: execute tasks in either synchronous multi-process mode or asynchronous event-loop mode, handling lifecycle orchestration, concurrency, and telemetry initialization.
- **TaskConfig**: the declarative configuration object for defining task execution parameters, connectors, services, and policies.

### Dependency Injection

Tasks are instantiated with all their dependencies already resolved. The framework supports **Dependency Injection** for both Connectors and Services.

When a Task is initialized, it receives:

1. **Connectors**: A dictionary of configured `Connector` instances (e.g., `input`, `output`).
2. **Services**: A dictionary of configured `Service` instances (e.g., `openai`, `database`).

This allows tasks to access external capabilities simply by calling methods on these injected objects (`self.openai_service.generate_text(...)`), without needing to instantiate clients or manage credentials.

### ProducerMixin semantics

The `ProducerMixin` enables outbound semantics within a task. A producer task receives pre-formed `Transaction` objects and dispatches them to external systems through connectors. The mixin requires tasks to implement `prepare_transaction(self, transaction: Transaction)`, which receives an immutable `Transaction` object. Inside this method, the task should typically dispatch the transaction to the appropriate connector (e.g., by calling `self.output_connector.dispatch_transactions([transaction])`).

The mixin manages the operational complexity of producing at scale (workers, timeouts, throttling). It does not attempt to understand the underlying transport protocol: once the producer dispatches a transaction, it is forwarded to the connector, which delegates transmission to its service.

The `ProducerPolicy` controls the execution, including:

- **Concurrency**: How many transactions to prepare/dispatch in parallel.
- **Batching**: Grouping transactions for processing.
- **Retries**: Configurable retry policies for the `prepare_transaction` step and success/exception handlers (`handle_prepare_success`, `handle_prepare_exception`).

### ConsumerMixin semantics

The Consumer subsystem of the Ergon Framework defines a deterministic, fault-tolerant, and fully observable execution pipeline for any component that consumes and processes transactions. Every transaction is executed inside a strict lifecycle governed by structured retry policies, timeout envelopes, exponential backoff, and automatic OpenTelemetry tracing. The lifecycle is always the same: the engine first attempts to fetch a batch of transactions from the given connector, then processes each transaction independently, and finally routes the outcome into either a success handler or an exception handler.

#### Lifecycle and Policy Control

Each stage—fetch, process, success, and exception—is isolated and governed by its own policy within the `ConsumerPolicy` structure.

The `ConsumerPolicy` defines:

- **Loop Policy (`ConsumerLoopPolicy`)**: Controls the main consumption loop, including:

  - **Batching (`BatchPolicy`)**: Batch size (`size`, `min_size`, `max_size`) and interval.
  - **Concurrency (`ConcurrencyPolicy`)**: Number of concurrent transactions to process (`value`, `min`, `max`).
  - **Empty Queue (`EmptyQueuePolicy`)**: Backoff behavior when no transactions are fetched (`backoff`, `backoff_multiplier`, `interval`).
  - **Timeouts**: Loop timeout, transaction processing timeout.
  - **Limits**: Max number of transactions to process.
  - **Streaming**: Boolean flag for streaming mode.
- **Step Policies**:

  - **Fetch (`FetchPolicy`)**: Controls transaction fetching (retry logic, connector name).
  - **Process (`ProcessPolicy`)**: Controls the `process_transaction` execution (retry logic).
  - **Success (`SuccessPolicy`)**: Controls the `handle_process_success` execution (retry logic).
  - **Exception (`ExceptionPolicy`)**: Controls the `handle_process_exception` execution (retry logic).

All step policies utilize a standard `RetryPolicy` which allows configuration of:

- `max_attempts`: Maximum number of retries.
- `timeout`: Timeout per attempt.
- `backoff`: Initial backoff delay.
- `backoff_multiplier`: Multiplier for exponential backoff.
- `backoff_cap`: Maximum backoff delay.

Instead of treating the lifecycle as a single block, the framework evaluates each component separately:

- **Fetch Phase**: Can retry under its own rules until the connector returns transactions or raises an unrecoverable error.
- **Process Phase**: Wraps the user-defined business logic (`process_transaction`). Business-category failures short-circuit the process phase and immediately transition into the exception handler without further attempts, while system-category failures are retried according to the process policy.
- **Success Phase**: When processing succeeds, the engine transitions into the success handler (`handle_process_success`), which can also fail and retry on its own terms. If the success handler exhausts its retries, the failure automatically falls back into the exception handler.
- **Exception Phase**: The exception handler itself (`handle_process_exception`) executes under its own retry envelope. If it ultimately fails after consuming its entire retry budget, the framework records a permanent handler failure and completes the transaction lifecycle deterministically.

#### Strict Timeout Enforcement

Timeout control is strict and guaranteed:

- **Synchronous**: Executed through a thread-isolated timeout mechanism.
- **Asynchronous**: Wrapped in an async timeout construct.

Any timeout triggers a structured `TransactionException` with category `TIMEOUT`, ensuring that no attempt can overrun the configured boundaries. All retry cycles use exponential backoff derived from the step policy, preventing aggressive retry loops under unstable conditions. The result is a pipeline that behaves predictably even under heavy load or transient external failures.

#### Observability

Every step and every attempt is fully instrumented with OpenTelemetry. The consumer automatically produces a hierarchical span structure: a span for the entire consumption loop, spans for each batch fetch, spans for each transaction lifecycle, and nested attempt-level spans for fetch attempts, process attempts, success attempts, and exception attempts. These spans include attributes such as attempt counters, timeouts, transaction identifiers, and active policy settings, which allows developers and operators to reconstruct the entire execution path in any observability backend.

#### Concurrency and The Loop

Execution can be synchronous or asynchronous:

- **Synchronous**: Uses a thread pool with configurable concurrency; each transaction is processed in its own worker thread, guaranteeing isolation from other workloads running in the same consumer loop.
- **Asynchronous**: Uses a semaphore to enforce concurrency limits, enabling highly parallel, high-throughput pipelines without blocking the event loop.

Both models support streaming and non-streaming modes: in streaming mode, empty fetches trigger exponential sleep intervals instead of terminating, allowing the consumer to continuously poll upstream sources without overwhelming them.

In summary, the Consumer Mixin transforms arbitrary transaction handling into a robust processing engine with automatic retries, automatic timeouts, exponential backoff, structured observability, concurrency control, and deterministic failure handling. The design ensures that business logic remains simple while the framework guarantees reliability, stability, and complete traceability across the entire lifecycle of every transaction.

### HybridTask semantics

Hybrid tasks combine both `ProducerMixin` and `ConsumerMixin` within a single execution unit. This pattern enables end-to-end workflows in which a task consumes one atomic transaction, transforms it, enriches it (potentially using injected **Services**), and then produces one or multiple outbound transactions to a different connector.

Hybrid tasks maintain atomicity in both directions. The inbound transaction is treated as indivisible; the domain logic runs once per atomic unit. Any resulting outbound transactions must each be created with the same indivisibility guarantee.

### Concurrency model

The Tasks Module supports three concurrency paradigms. Synchronous tasks can be executed using multi-process parallelism for CPU-bound workloads and multi-thread parallelism for I/O-bound workloads. Asynchronous tasks execute through an event loop capable of supporting a very high number of concurrent operations with minimal overhead.

### Execution Primitives

The Tasks Module provides execution primitives that can be used directly in user-defined methods and services. These primitives encapsulate common execution patterns with automatic retry logic, timeout handling, context propagation, and observability.

**`run_fn`** is a primitive that wraps function execution with:

- Automatic retry logic with configurable policies
- Timeout enforcement
- OpenTelemetry context propagation and tracing
- Structured error handling

It can be used both as a function call and as a decorator, making it suitable for wrapping service methods, task helper functions, or any operation that requires resilience and observability.

**`run_concurrently`** is a primitive for concurrent execution patterns that:

- Manages a pool of concurrent operations with configurable limits
- Implements a refill pattern to maintain concurrency levels
- Handles timeouts and error collection
- Returns results and completion counts

These primitives are available for use in:

- Service implementations to wrap external API calls
- Task helper methods that require retry logic
- Any user-defined code that needs framework-level resilience and observability

By using these primitives, developers can ensure that their custom code benefits from the same retry policies, timeout guarantees, and observability instrumentation that the framework provides for built-in operations.

### Connector and Service Dependencies

Tasks operate within a strict dependency model:

- **Connectors**: Used for *flow* (inbound/outbound transactions).
- **Services**: Used for *capabilities* (enrichment, lookups, API calls).

Tasks hold domain logic. Connectors coordinate transaction movement. Services own protocol and transport mechanics. This clean layering prevents architectural drift and keeps the system maintainable as new transport types are introduced.

### Observability

Observability is injected into tasks automatically by runners. Logging, tracing, and metrics providers are initialized before the task is constructed. Root spans, execution metadata, and connector instantiation spans are created automatically. Context propagation ensures trace identifiers and span identifiers are included in logs without explicit handling inside task code.

### Task lifecycle

Every task follows a deterministic lifecycle. TaskManager registers configurations, resolves the correct runner, and launches workers. Each worker initializes telemetry, constructs connectors and services, and instantiates the task with all required metadata (via dependency injection). Synchronous runners call the task’s execute method; asynchronous runners await the async execute method. After execution, the task’s exit routine flushes telemetry, closes connectors, and ensures clean shutdown.

### Scalability model

The scalability model operates across multiple axes. Process-level scaling supports horizontal parallelism with isolated workers. Thread-level scaling increases throughput for synchronous I/O. Async scaling enables large numbers of concurrent network operations with minimal resource usage. Connector-level scaling determines how incoming work is partitioned, providing sharding, consumer groups, and queue/topic level concurrency.

### Summary

In conclusion, the Tasks Module is the computational and architectural backbone of the Ergon Framework. It defines how units of work are represented, how they are processed, and how they interact with external systems. By enforcing a transaction-first philosophy, strict separation of concerns, and dependency injection, the module provides a clean, powerful, and production-ready foundation for building high-throughput automation pipelines.
